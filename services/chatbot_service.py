import re
from sklearn.metrics.pairwise import cosine_similarity
from nomic import embed

class ChatbotService:
    def __init__(self, config, ai_models, email_service):
        self.config = config
        self.ai_models = ai_models
        self.email_service = email_service
        
        # ì±—ë´‡ ì˜ë„ ë¶„ë¥˜ìš© ë¼ë²¨
        self.candidate_labels = [
            "correct the vocabulary, spelling",
            "image generation using text", 
            "find something",
            "email search for a person"
        ]
        
        # í•œêµ­ì–´ íŒ¨í„´ ë§¤ì¹­
        self.korean_patterns = {
            "grammar": {
                "keywords": ["êµì •", "ë§ì¶¤ë²•", "ë¬¸ë²•", "í‹€ë ¸", "ê³ ì³", "ìˆ˜ì •"],
                "action": "grammar_correction"
            },
            "image": {
                "keywords": ["ì´ë¯¸ì§€", "ê·¸ë¦¼", "ì‚¬ì§„", "ê·¸ë ¤", "ë§Œë“¤ì–´", "ìƒì„±"],
                "action": "image_generation"
            },
            "person_search": {
                "keywords": ["ë‹˜", "ì”¨"],
                "required": ["ë©”ì¼", "ì´ë©”ì¼"],
                "action": "person_search"
            },
            "general_search": {
                "keywords": ["ì°¾ì•„", "ê²€ìƒ‰", "ì°¾ê¸°"],
                "action": "email_search"
            }
        }
    
    def process_user_input(self, user_input, user_email, app_password):
        """ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬"""
        try:
            print(f"[ğŸ¤– ì±—ë´‡ ìš”ì²­] ì‚¬ìš©ì: {user_email}, ì…ë ¥: {user_input}")
            
            if not user_input:
                return {"error": "ì…ë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}, 400
            
            # ì˜ë„ ë¶„ì„
            intent_result = self._analyze_intent(user_input)
            
            print(f"[ğŸ¯ ì˜ë„ ë¶„ì„] {intent_result['action']} (ì‹ ë¢°ë„: {intent_result['confidence']:.3f})")
            
            # ê¸°ëŠ¥ë³„ ì‹¤í–‰
            if intent_result['action'] == "grammar_correction":
                response = self._handle_grammar_correction(user_input)
            elif intent_result['action'] == "image_generation":
                response = self._handle_image_generation(user_input)
            elif intent_result['action'] == "email_search":
                response = self._handle_general_search(user_input, user_email, app_password)
            elif intent_result['action'] == "person_search":
                response = self._handle_person_search(user_input, user_email, app_password)
            else:
                response = self._handle_unknown_intent()
            
            return {
                "response": response,
                "action": intent_result['action'],
                "confidence": float(intent_result['confidence']),
                "detected_intent": intent_result['action'],
                "detection_method": intent_result['method']
            }, 200
            
        except Exception as e:
            print(f"[â—ì±—ë´‡ ì˜¤ë¥˜] {str(e)}")
            return {"error": str(e)}, 500
    
    def _analyze_intent(self, user_input):
        """ì˜ë„ ë¶„ì„ (ì˜ì–´ embedding + í•œêµ­ì–´ í‚¤ì›Œë“œ)"""
        # 1. ì˜ì–´ Embedding ê¸°ë°˜ ë¶„ë¥˜
        try:
            text_inputs = [user_input] + self.candidate_labels
            result = embed.text(text_inputs, model='nomic-embed-text-v1', task_type='classification')
            
            embedding_list = result['embeddings']
            email_embedding = [embedding_list[0]]
            label_embeddings = embedding_list[1:]
            
            scores = cosine_similarity(email_embedding, label_embeddings)[0]
            best_index = scores.argmax()
            embedding_score = scores[best_index]
            embedding_label = self.candidate_labels[best_index]
            
        except Exception as e:
            print(f"[âš ï¸ Embedding ë¶„ë¥˜ ì‹¤íŒ¨] {str(e)}")
            embedding_score = 0.0
            embedding_label = "unknown"
        
        # 2. í•œêµ­ì–´ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        korean_result = self._analyze_korean_patterns(user_input)
        
        # 3. ìµœì¢… ì˜ë„ ê²°ì •
        embedding_action_map = {
            "correct the vocabulary, spelling": "grammar_correction",
            "image generation using text": "image_generation", 
            "find something": "email_search",
            "email search for a person": "person_search"
        }
        
        embedding_action = embedding_action_map.get(embedding_label, "unknown")
        embedding_threshold = 0.25
        
        # ìµœì¢… ê²°ì •
        if korean_result["confidence"] >= 0.3 and korean_result["confidence"] > embedding_score:
            return {
                'action': korean_result["action"],
                'confidence': korean_result["confidence"],
                'method': 'korean_keywords'
            }
        elif embedding_score >= embedding_threshold:
            return {
                'action': embedding_action,
                'confidence': embedding_score,
                'method': 'english_embedding'
            }
        else:
            return {
                'action': 'unknown',
                'confidence': max(korean_result["confidence"], embedding_score),
                'method': 'low_confidence'
            }
    
    def _analyze_korean_patterns(self, user_input):
        """í•œêµ­ì–´ íŒ¨í„´ ë¶„ì„"""
        user_input_lower = user_input.lower()
        
        korean_result = {"action": None, "confidence": 0.0, "matched_keywords": []}
        
        for pattern_name, pattern_info in self.korean_patterns.items():
            matched_keywords = []
            
            # ì¼ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in pattern_info["keywords"]:
                if keyword in user_input_lower:
                    matched_keywords.append(keyword)
            
            # í•„ìˆ˜ í‚¤ì›Œë“œ í™•ì¸ (person_searchìš©)
            if "required" in pattern_info:
                required_found = any(req in user_input_lower for req in pattern_info["required"])
                if not required_found:
                    continue
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            if matched_keywords:
                confidence = len(matched_keywords) / len(pattern_info["keywords"])
                
                # person_searchëŠ” íŠ¹ë³„ ì²˜ë¦¬
                if pattern_name == "person_search" and "required" in pattern_info:
                    confidence += 0.3
                
                if confidence > korean_result["confidence"]:
                    korean_result = {
                        "action": pattern_info["action"],
                        "confidence": confidence,
                        "matched_keywords": matched_keywords
                    }
        
        return korean_result
    
    def _handle_grammar_correction(self, user_input):
        """ë¬¸ë²• êµì • ì²˜ë¦¬"""
        try:
            # êµì •í•  í…ìŠ¤íŠ¸ ì¶”ì¶œ
            correction_text = user_input
            remove_words = ["êµì •í•´ì£¼ì„¸ìš”", "êµì •í•´ì¤˜", "ë§ì¶¤ë²•", "ë¬¸ë²•", "correct", "spelling", "check", "fix"]
            for word in remove_words:
                correction_text = correction_text.replace(word, "").strip()
            
            if not correction_text:
                return "ğŸ“ **ë¬¸ë²• ë° ë§ì¶¤ë²• êµì •**\n\nêµì •í•˜ê³  ì‹¶ì€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ: 'ì•ˆë…•í•˜ì„¸ìš”. ì œê°€ ì˜¤ëŠ˜ íšŒì˜ì— ì°¸ì„ëª»í• ê²ƒ ê°™ìŠµë‹ˆë‹¤' êµì •í•´ì£¼ì„¸ìš”"
            
            # HuggingFace API ì‚¬ìš©
            if not self.config.HF_TOKEN:
                return f"ğŸ“ **ë¬¸ë²• êµì • ê²°ê³¼**\n\nì›ë³¸: {correction_text}\n\nâš ï¸ HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•„ êµì • ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            try:
                client = self.ai_models.get_inference_client()
                
                prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•, ë¬¸ë²•, ë„ì–´ì“°ê¸°ë¥¼ êµì •í•´ì£¼ì„¸ìš”.

ì›ë³¸ í…ìŠ¤íŠ¸:
"{correction_text}"

êµì • ì§€ì¹¨:
1. ë§ì¶¤ë²• ì˜¤ë¥˜ ìˆ˜ì •
2. ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ì •  
3. ë„ì–´ì“°ê¸° ìˆ˜ì •
4. ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ìœ¼ë¡œ ê°œì„ 
5. ì›ë˜ ì˜ë¯¸ëŠ” ìœ ì§€

êµì •ëœ í…ìŠ¤íŠ¸:"""
                
                messages = [
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ êµì • í¸ì§‘ìì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ]
                
                response = client.chat_completion(
                    messages=messages,
                    max_tokens=300,
                    temperature=0.3
                )
                
                corrected_text = response.choices[0].message.content.strip()
                
                return f"""ğŸ“ **ë¬¸ë²• ë° ë§ì¶¤ë²• êµì • ì™„ë£Œ**

**ì›ë³¸:**
{correction_text}

**êµì •ëœ í…ìŠ¤íŠ¸:**
{corrected_text}

âœ… **AI êµì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**"""
                
            except Exception as e:
                # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ êµì •ìœ¼ë¡œ fallback
                return self._simple_grammar_correction(correction_text)
                
        except Exception as e:
            return "âŒ ë¬¸ë²• êµì • ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _simple_grammar_correction(self, text):
        """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ êµì •"""
        simple_corrections = {
            "ë°ì´íƒ€": "ë°ì´í„°", "ì»´í“¨íƒ€": "ì»´í“¨í„°", "ì…‹íŒ…": "ì„¤ì •",
            "ë¯¸íŒ…": "íšŒì˜", "í•´ì•¼ë˜ëŠ”": "í•´ì•¼ í•˜ëŠ”", "í• ìˆ˜ìˆëŠ”": "í•  ìˆ˜ ìˆëŠ”",
            "ëª»í• ê²ƒ": "ëª»í•  ê²ƒ", "ì°¸ì„ëª»í• ": "ì°¸ì„í•˜ì§€ ëª»í• "
        }
        
        corrected_simple = text
        applied_corrections = []
        
        for wrong, correct in simple_corrections.items():
            if wrong in corrected_simple:
                corrected_simple = corrected_simple.replace(wrong, correct)
                applied_corrections.append(f"'{wrong}' â†’ '{correct}'")
        
        if applied_corrections:
            return f"""ğŸ“ **ê°„ë‹¨ ë§ì¶¤ë²• êµì •**

**ì›ë³¸:** {text}
**êµì •ëœ í…ìŠ¤íŠ¸:** {corrected_simple}

**ì ìš©ëœ êµì •:**
{chr(10).join('â€¢ ' + correction for correction in applied_corrections)}"""
        else:
            return f"ğŸ“ **êµì • ê²€í†  ì™„ë£Œ**\n\ní˜„ì¬ í…ìŠ¤íŠ¸ì—ì„œ ëª…ë°±í•œ ì˜¤ë¥˜ë¥¼ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    
    def _handle_image_generation(self, user_input):
        """ì´ë¯¸ì§€ ìƒì„± ì²˜ë¦¬"""
        try:
            # í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
            image_prompt = user_input
            remove_words = ["ì´ë¯¸ì§€ ìƒì„±í•´ì£¼ì„¸ìš”", "ì´ë¯¸ì§€ ìƒì„±", "ê·¸ë ¤ì¤˜", "ê·¸ë¦¼", "image generation", "generate", "ë§Œë“¤ì–´"]
            for word in remove_words:
                image_prompt = image_prompt.replace(word, "").strip()
            
            if not image_prompt:
                return "ğŸ¨ **ì´ë¯¸ì§€ ìƒì„±**\n\nìƒì„±í•˜ê³  ì‹¶ì€ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ:\nâ€¢ 'ì•„ë¦„ë‹¤ìš´ ì„ì–‘ê³¼ ë°”ë‹¤'\nâ€¢ 'ê·€ì—¬ìš´ ê³ ì–‘ì´ê°€ ë†€ê³  ìˆëŠ” ëª¨ìŠµ'"
            
            if not self.config.HF_TOKEN:
                return f"ğŸ¨ **ì´ë¯¸ì§€ ìƒì„±**\n\nìš”ì²­ëœ ì´ë¯¸ì§€: '{image_prompt}'\n\nâš ï¸ HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ì´ë¯¸ì§€ ìƒì„±ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            try:
                from huggingface_hub import InferenceClient
                import base64
                import time
                import os
                
                client = InferenceClient(
                    model="runwayml/stable-diffusion-v1-5",
                    token=self.config.HF_TOKEN
                )
                
                # í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­
                enhanced_prompt = self._translate_korean_to_english(image_prompt)
                enhanced_prompt = f"{enhanced_prompt}, high quality, detailed, beautiful, artistic"
                
                # ì´ë¯¸ì§€ ìƒì„±
                image_bytes = client.text_to_image(
                    prompt=enhanced_prompt,
                    height=512,
                    width=512,
                    num_inference_steps=20
                )
                
                # íŒŒì¼ ì €ì¥
                timestamp = int(time.time())
                filename = f"generated_image_{timestamp}.png"
                filepath = os.path.join(self.config.ATTACHMENT_FOLDER, filename)
                
                os.makedirs(self.config.ATTACHMENT_FOLDER, exist_ok=True)
                
                with open(filepath, "wb") as f:
                    f.write(image_bytes)
                
                return f"""ğŸ¨ **ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!**

ğŸ“ **ìš”ì²­:** '{image_prompt}'
ğŸ–¼ï¸ **ìƒì„±ëœ ì´ë¯¸ì§€:** {filename}
ğŸ“ **ì €ì¥ ìœ„ì¹˜:** /static/attachments/{filename}
ğŸŒ **ì›¹ ì£¼ì†Œ:** http://localhost:5001/static/attachments/{filename}

âœ… **ì„±ê³µ!** ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."""
                
            except Exception as e:
                return f"ğŸ¨ **ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨**\n\nì˜¤ë¥˜: {str(e)}\n\nğŸ’¡ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                
        except Exception as e:
            return "âŒ ì´ë¯¸ì§€ ìƒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _translate_korean_to_english(self, text):
        """í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
        korean_to_english = {
            "ê³ ì–‘ì´": "cute cat", "ê°•ì•„ì§€": "cute dog", "ê½ƒ": "beautiful flowers",
            "ë°”ë‹¤": "ocean and waves", "ì‚°": "mountains and nature", "ì„ì–‘": "beautiful sunset",
            "í•˜ëŠ˜": "blue sky with clouds", "ìˆ²": "forest and trees", "ë„ì‹œ": "modern city",
            "ìë™ì°¨": "modern car", "ì§‘": "beautiful house", "ì‚¬ëŒ": "person"
        }
        
        english_text = text
        for korean, english in korean_to_english.items():
            if korean in text:
                english_text = english_text.replace(korean, english)
        
        # í•œêµ­ì–´ê°€ ë‚¨ì•„ìˆìœ¼ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if any(ord(char) > 127 for char in english_text):
            english_text = f"a beautiful {text}"
        
        return english_text
    
    def _handle_general_search(self, user_input, user_email, app_password):
        """ì¼ë°˜ ì´ë©”ì¼ ê²€ìƒ‰"""
        try:
            # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
            search_keywords = user_input.lower()
            remove_words = ["ì°¾ì•„ì¤˜", "ì°¾ì•„ì£¼ì„¸ìš”", "ê²€ìƒ‰í•´ì¤˜", "ê²€ìƒ‰", "find", "search", "ë©”ì¼", "ì´ë©”ì¼", "email"]
            for word in remove_words:
                search_keywords = search_keywords.replace(word, "").strip()
            
            if not search_keywords:
                return "ğŸ” **ë©”ì¼ ê²€ìƒ‰**\n\nê²€ìƒ‰í•˜ê³  ì‹¶ì€ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ:\nâ€¢ 'íšŒì˜ ê´€ë ¨ ë©”ì¼ ì°¾ì•„ì¤˜'\nâ€¢ 'í”„ë¡œì íŠ¸ ì—…ë°ì´íŠ¸ ê²€ìƒ‰'"
            
            # ì´ë©”ì¼ ê²€ìƒ‰ ì‹¤í–‰
            try:
                found_emails = self.email_service.search_emails(user_email, app_password, search_keywords, max_results=50)
                
                if found_emails:
                    result = f"ğŸ” **ê²€ìƒ‰ ê²°ê³¼**\n\ní‚¤ì›Œë“œ: '{search_keywords}'\nê²€ìƒ‰ëœ ë©”ì¼: {len(found_emails)}ê°œ\n\n"
                    for i, mail_info in enumerate(found_emails, 1):
                        result += f"**{i}. {mail_info['subject']}**\n"
                        result += f"ğŸ“¤ {mail_info['from']}\n"
                        result += f"ğŸ“… {mail_info['date']}\n"
                        if mail_info['preview']:
                            result += f"ğŸ’¬ {mail_info['preview']}\n"
                        result += "\n"
                    result += "ğŸ’¡ ë” ì •í™•í•œ ê²€ìƒ‰ì„ ìœ„í•´ êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”."
                    return result
                else:
                    return f"ğŸ” **ê²€ìƒ‰ ê²°ê³¼**\n\ní‚¤ì›Œë“œ: '{search_keywords}'\n\nâŒ ê´€ë ¨ëœ ë©”ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”."
                    
            except Exception as e:
                return f"âŒ ë©”ì¼ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì˜¤ë¥˜: {str(e)}"
                
        except Exception as e:
            return "âŒ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _handle_person_search(self, user_input, user_email, app_password):
        """íŠ¹ì • ì‚¬ëŒ ë©”ì¼ ê²€ìƒ‰"""
        try:
            # Qwenìœ¼ë¡œ ì‚¬ëŒ ì´ë¦„/ì´ë©”ì¼ ì¶”ì¶œ
            search_target = self._extract_search_target_with_qwen(user_input)
            
            if not search_target or len(search_target.strip()) < 2:
                # ê°„ë‹¨í•œ ì¶”ì¶œ ë°©ë²•
                words = user_input.split()
                potential_targets = []
                
                for word in words:
                    if "@" in word and "." in word:  # ì´ë©”ì¼ ì£¼ì†Œ
                        potential_targets.append(word)
                    elif len(word) >= 2 and len(word) <= 4 and word.replace(" ", "").isalpha():  # í•œêµ­ì–´ ì´ë¦„
                        potential_targets.append(word)
                
                if potential_targets:
                    search_target = potential_targets[0]
                else:
                    return "ğŸ‘¤ **ì‚¬ëŒë³„ ë©”ì¼ ê²€ìƒ‰**\n\nì°¾ê³  ì‹¶ì€ ì‚¬ëŒì˜ ì´ë¦„ì´ë‚˜ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ëª…í™•íˆ ì•Œë ¤ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ:\nâ€¢ 'ê¹€ì² ìˆ˜ë‹˜ì˜ ë©”ì¼'\nâ€¢ 'john@company.com ë©”ì¼'"
            
            try:
                # ì‚¬ëŒë³„ ì´ë©”ì¼ ê²€ìƒ‰ ì‹¤í–‰
                found_emails = self.email_service.search_emails(user_email, app_password, search_target, max_results=100)
                
                # ë°œì‹ ì ì •ë³´ë¡œ í•„í„°ë§
                person_emails = []
                search_lower = search_target.lower()
                
                for email_info in found_emails:
                    from_field = email_info['from'].lower()
                    if (search_lower in from_field or 
                        any(part.strip() in from_field for part in search_lower.split() if part.strip())):
                        person_emails.append(email_info)
                        
                        if len(person_emails) >= 10:
                            break
                
                if person_emails:
                    result = f"ğŸ‘¤ **ì‚¬ëŒë³„ ë©”ì¼ ê²€ìƒ‰ ê²°ê³¼**\n\nê²€ìƒ‰ ëŒ€ìƒ: '{search_target}'\në°œê²¬ëœ ë©”ì¼: {len(person_emails)}ê°œ\n\n"
                    for i, mail_info in enumerate(person_emails, 1):
                        result += f"**{i}. {mail_info['subject']}**\n"
                        result += f"ğŸ“¤ {mail_info['from']}\n"
                        result += f"ğŸ“… {mail_info['date']}\n\n"
                    result += "ğŸ’¡ íŠ¹ì • ë©”ì¼ì„ ìì„¸íˆ ë³´ë ¤ë©´ ë©”ì¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”."
                    return result
                else:
                    return f"ğŸ‘¤ **ì‚¬ëŒë³„ ë©”ì¼ ê²€ìƒ‰ ê²°ê³¼**\n\nê²€ìƒ‰ ëŒ€ìƒ: '{search_target}'\n\nâŒ í•´ë‹¹ ì‚¬ëŒì˜ ë©”ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ ì •í™•í•œ ì´ë¦„ì´ë‚˜ ì´ë©”ì¼ ì£¼ì†Œë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”."
                    
            except Exception as e:
                return f"âŒ ì‚¬ëŒë³„ ë©”ì¼ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì˜¤ë¥˜: {str(e)}"
                
        except Exception as e:
            return "âŒ ì‚¬ëŒ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _extract_search_target_with_qwen(self, text):
        """Qwenì„ ì´ìš©í•˜ì—¬ ê²€ìƒ‰ ëŒ€ìƒ ì¶”ì¶œ"""
        # Qwen ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¡œë”© ì‹œë„
        if not self.ai_models.load_qwen_model():
            print("[âš ï¸ Qwen ëª¨ë¸ ì—†ìŒ - ê°„ë‹¨ ì¶”ì¶œ ì‚¬ìš©]")
            words = text.split()
            return " ".join(words[-2:]) if len(words) >= 2 else text
        
        try:
            prompt = (
                "<|im_start|>system\nYou are an email assistant. "
                "Your job is to extract the email address or name the user is referring to. "
                "You must always respond in the format: The user is referring to ... \n"
                "<|im_end|>\n"
                f"<|im_start|>user\n{text}<|im_end|>\n"
                "<|im_start|>assistant\n"
            )
            
            inputs = self.ai_models.qwen_tokenizer(prompt, return_tensors="pt").to(self.ai_models.qwen_model.device)
            
            with torch.no_grad():
                outputs = self.ai_models.qwen_model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=False,
                    eos_token_id=self.ai_models.qwen_tokenizer.eos_token_id
                )
            
            decoded_output = self.ai_models.qwen_tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # "assistant" ì´í›„ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜´
            if "assistant" in decoded_output:
                after_assistant = decoded_output.split("assistant")[-1].strip()
                prefix = "The user is referring to "
                if prefix in after_assistant:
                    result = after_assistant.split(prefix)[-1].strip().rstrip(".").strip('"')
                    return result
            
            return text
            
        except Exception as e:
            print(f"[âš ï¸ Qwen ì¶”ì¶œ ì˜¤ë¥˜] {str(e)}")
            # ì˜¤ë¥˜ ì‹œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œë¡œ fallback
            words = text.split()
            return " ".join(words[-2:]) if len(words) >= 2 else text
    
    def _handle_unknown_intent(self):
        """ì•Œ ìˆ˜ ì—†ëŠ” ì˜ë„ ì²˜ë¦¬"""
        return """â“ ìš”ì²­ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‘œí˜„ì„ ì‹œë„í•´ì£¼ì„¸ìš”.

ğŸ”§ **ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥ë“¤:**
â€¢ **ë¬¸ë²•/ë§ì¶¤ë²• êµì •**: "ì´ ë¬¸ì¥ êµì •í•´ì£¼ì„¸ìš”" / "correct this sentence"
â€¢ **ì´ë¯¸ì§€ ìƒì„±**: "ê³ ì–‘ì´ ê·¸ë¦¼ ê·¸ë ¤ì¤˜" / "generate cat image"  
â€¢ **ë©”ì¼ ê²€ìƒ‰**: "íšŒì˜ ê´€ë ¨ ë©”ì¼ ì°¾ì•„ì¤˜" / "find meeting emails"
â€¢ **ì‚¬ëŒë³„ ë©”ì¼**: "ê¹€ì² ìˆ˜ë‹˜ ë©”ì¼ ê²€ìƒ‰" / "search john@company.com emails"

ğŸ’¡ **Example / ì˜ˆì‹œ:**
- í•œêµ­ì–´: "ì•ˆë…•í•˜ì„¸ìš”. ì œê°€ ì˜¤ëŠ˜ íšŒì˜ì— ì°¸ì„ëª»í• ê²ƒ ê°™ìŠµë‹ˆë‹¤ êµì •í•´ì£¼ì„¸ìš”"
- English: "correct the grammar: I can't attend meeting today"
- í˜¼í•©: "find í”„ë¡œì íŠ¸ ê´€ë ¨ emails" """

    def generate_ai_reply(self, sender, subject, body, current_user_email):
        """AI ë‹µì¥ ìƒì„±"""
        try:
            print(f"[ğŸ¤– AI ë‹µì¥ ìš”ì²­] User: {current_user_email}, From: {sender}")
            
            if not self.config.HF_TOKEN:
                return {'error': 'HF_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.'}, 500
            
            client = self.ai_models.get_inference_client()
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            user_prompt = self._build_ai_reply_prompt(sender, subject, body)
            
            messages = [
                {"role": "system", "content": "You are a helpful email assistant that writes professional email replies."},
                {"role": "user", "content": user_prompt}
            ]
            
            response = client.chat_completion(
                messages=messages,
                max_tokens=256,
                temperature=0.7
            )
            
            ai_reply = response.choices[0].message.content.strip()
            
            print(f"[âœ… AI ë‹µì¥ ìƒì„± ì™„ë£Œ] User: {current_user_email}, ê¸¸ì´: {len(ai_reply)}ì")
            
            return {'success': True, 'ai_reply': ai_reply}, 200
            
        except Exception as e:
            print(f"[â—AI ë‹µì¥ ìƒì„± ì‹¤íŒ¨] {str(e)}")
            return {'error': f'AI ë‹µì¥ ìƒì„± ì‹¤íŒ¨: {str(e)}'}, 500
    
    def _build_ai_reply_prompt(self, sender, subject, body):
        """AI ë‹µì¥ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""
You are a helpful email assistant that writes professional email replies.

Please read the following email and write a polite, professional reply in English:

---
From: {sender}
Subject: {subject}
Body: {body}
---

Instructions:
1. Identify the purpose of the email (invitation, question, information request, scheduling, etc.)
2. Write a concise (3-4 sentences), polite reply that directly addresses the purpose
3. Use a friendly yet professional tone
4. Only output the reply text (no analysis, no quotes, no original email content)

Reply:
""".strip()